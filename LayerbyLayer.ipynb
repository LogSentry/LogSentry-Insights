{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uR3rx_EPi6E"
      },
      "source": [
        "# Layer by Layer approach for bulding a MLP (Multi-Layer Perseptron) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aqb96IjYPi6H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "\n",
        "# Base layer class: A generic layer in the neural network\n",
        "class Layer:\n",
        "    def __init__(self, input, output):\n",
        "        \"\"\"\n",
        "        Initialize the Layer class.\n",
        "\n",
        "        Parameters:\n",
        "        input (np.ndarray): Input data to this layer.\n",
        "        output (np.ndarray): Output data from this layer.\n",
        "        \"\"\"\n",
        "        self.input = input   # Stores the input to this layer (type: np.ndarray)\n",
        "        self.output = output # Stores the output from this layer (type: np.ndarray)\n",
        "\n",
        "    def forward(self, output):\n",
        "        \"\"\"\n",
        "        Defines the forward pass through the layer.\n",
        "\n",
        "        This method should be implemented by subclasses to specify how the input data\n",
        "        is transformed into output data using the layer's parameters.\n",
        "\n",
        "        Parameters:\n",
        "        output (np.ndarray): Input data to be processed by the layer.\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Output data after applying the layer's transformation.\n",
        "        \"\"\"\n",
        "        pass  # To be implemented by subclasses, defines how the input is transformed to output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        \"\"\"\n",
        "        Defines the backward pass through the layer.\n",
        "\n",
        "        This method should be implemented by subclasses to specify how the gradients\n",
        "        of the loss function with respect to the output are used to update the layer's\n",
        "        parameters and compute the gradient with respect to the input data.\n",
        "\n",
        "        Parameters:\n",
        "        output_gradient (np.ndarray): Gradient of the loss function with respect to the layer's output.\n",
        "        learning_rate (float): Learning rate for updating the layer's parameters.\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Gradient of the loss function with respect to the layer's input data.\n",
        "        \"\"\"\n",
        "        pass  # To be implemented by subclasses, defines how gradients are used to update parameters\n",
        "\n",
        "# Dense layer class: A fully connected layer\n",
        "class Dense(Layer):\n",
        "    def __init__(self, no_of_perseptrons):\n",
        "        \"\"\"\n",
        "        Initialize the Dense layer with a given number of neurons.\n",
        "\n",
        "        Parameters:\n",
        "        output_size (int): Number of neurons (perceptrons) in this layer.\n",
        "        \"\"\"\n",
        "        self.no_of_perseptrons = no_of_perseptrons  # Number of neurons in the layer (type: int)\n",
        "        self.input_size = None  # Input size will be determined during the forward pass (type: int or None)\n",
        "        self.weights = None  # Weights for the layer's connections (type: np.ndarray or None)\n",
        "        self.bias = None  # Biases for the layer (type: np.ndarray or None)\n",
        "        self.output_size = no_of_perseptrons\n",
        "\n",
        "        #todo: add logic to change  none types to actual values\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform the forward pass through the dense layer.\n",
        "\n",
        "        Computes the output of the dense layer by applying a linear transformation to the\n",
        "        input data using the layer's weights and biases.\n",
        "\n",
        "        Parameters:\n",
        "        input (np.ndarray): Input data to the layer (shape: [input_size, 1]).\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Output data from the layer after applying the linear transformation (shape: [output_size, 1]).\n",
        "        \"\"\"\n",
        "        if input.ndim == 1:\n",
        "            input = input.reshape(-1, 1)\n",
        "        elif input.ndim > 2:\n",
        "            raise ValueError(\"Input must be 1D or 2D array\")\n",
        "\n",
        "        if self.input_size is None:\n",
        "            self.input_size = input.shape[0]\n",
        "            self.weights = np.random.randn(self.no_of_perseptrons, self.input_size) * np.sqrt(2.0 / (self.input_size + self.no_of_perseptrons))\n",
        "            self.bias = np.zeros((self.no_of_perseptrons, 1))\n",
        "\n",
        "        self.input = input\n",
        "        self.output = np.dot(self.weights, self.input) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    def updater(self, input_size):\n",
        "        \"\"\"\n",
        "        Initialize the weights and biases for the dense layer.\n",
        "\n",
        "        Parameters:\n",
        "        input_size (int): Number of input features.\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.output_size = self.no_of_perseptrons\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        \"\"\"\n",
        "        Perform the backward pass through the dense layer and update its parameters.\n",
        "\n",
        "        Computes the gradients of the loss function with respect to the layer's weights\n",
        "        and biases, updates these parameters using the provided learning rate, and calculates\n",
        "        the gradient with respect to the input data.\n",
        "\n",
        "        Parameters:\n",
        "        output_gradient (np.ndarray): Gradient of the loss function with respect to the layer's output (shape: [output_size, 1]).\n",
        "        learning_rate (float): Learning rate for updating the weights and biases.\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Gradient of the loss function with respect to the input data (shape: [input_size, 1]).\n",
        "        \"\"\"\n",
        "        if output_gradient.ndim == 1:\n",
        "            output_gradient = output_gradient.reshape(-1, 1)\n",
        "\n",
        "        self.old_weights = self.weights\n",
        "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.bias -= learning_rate * output_gradient\n",
        "        input_gradient = np.dot(self.old_weights.T, output_gradient)\n",
        "        return input_gradient.reshape(self.input.shape)\n",
        "\n",
        "# Activation class: Applies an activation function\n",
        "class Activation:\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        \"\"\"\n",
        "        Initialize the Activation class with a specific activation function and its derivative.\n",
        "\n",
        "        Parameters:\n",
        "        activation (callable): Activation function to be applied (e.g., sigmoid, ReLU).\n",
        "        activation_prime (callable): Derivative of the activation function.\n",
        "        \"\"\"\n",
        "        self.activation = activation  # Activation function to be used (type: callable)\n",
        "        self.activation_prime = activation_prime  # Derivative of the activation function (type: callable)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform the forward pass through the activation function.\n",
        "\n",
        "        Applies the activation function to the input data and stores the result.\n",
        "\n",
        "        Parameters:\n",
        "        input (np.ndarray): Input data to the activation function (type: np.ndarray).\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Output data after applying the activation function (type: np.ndarray).\n",
        "        \"\"\"\n",
        "        self.input = input  # Save input for use in the backward pass (type: np.ndarray)\n",
        "        return self.activation(self.input)  # Apply activation function (type: np.ndarray)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        \"\"\"\n",
        "        Perform the backward pass through the activation function.\n",
        "\n",
        "        Computes the gradient of the loss function with respect to the input data\n",
        "        by applying the derivative of the activation function.\n",
        "\n",
        "        Parameters:\n",
        "        output_gradient (np.ndarray): Gradient of the loss function with respect to the activation function's output (type: np.ndarray).\n",
        "        learning_rate (float): Learning rate (not used in this method, but included for consistency).\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Gradient of the loss function with respect to the input data (type: np.ndarray).\n",
        "        \"\"\"\n",
        "        return np.multiply(output_gradient, self.activation_prime(self.input))  # Compute gradient with respect to the input data (type: np.ndarray)\n",
        "\n",
        "# Sigmoid activation class\n",
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the Sigmoid activation function and its derivative.\n",
        "        \"\"\"\n",
        "        # Define sigmoid function and its derivative\n",
        "        sigmoid = staticmethod(lambda x: 1 / (1 + np.exp(-x)))  # Sigmoid function (type: callable)\n",
        "        sigmoid_prime = staticmethod(lambda x: (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x)))))  # Derivative of sigmoid function (type: callable)\n",
        "        super().__init__(sigmoid, sigmoid_prime)  # Initialize parent class with sigmoid functions\n",
        "\n",
        "# Loss class: Provides loss function and its derivative\n",
        "class Loss(Layer):\n",
        "    @staticmethod\n",
        "    def mse(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate the Mean Squared Error (MSE) loss between the true labels and predicted labels.\n",
        "\n",
        "        Parameters:\n",
        "        y_true (np.ndarray): True labels (shape: [n_samples, 1]).\n",
        "        y_pred (np.ndarray): Predicted labels (shape: [n_samples, 1]).\n",
        "\n",
        "        Returns:\n",
        "        float: MSE loss value indicating the average squared difference between true and predicted labels.\n",
        "        \"\"\"\n",
        "        return np.mean(np.power(y_true - y_pred, 2))  # Mean Squared Error loss (type: float)\n",
        "\n",
        "    @staticmethod\n",
        "    def mse_derivative(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate the derivative of the Mean Squared Error (MSE) loss function with respect to the predictions.\n",
        "\n",
        "        Parameters:\n",
        "        y_true (np.ndarray): True labels (shape: [n_samples, 1]).\n",
        "        y_pred (np.ndarray): Predicted labels (shape: [n_samples, 1]).\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Gradient of MSE loss with respect to predictions (shape: [n_samples, 1]).\n",
        "        \"\"\"\n",
        "        return 2 * (y_pred - y_true) / np.size(y_true)  # Derivative of MSE loss (type: np.ndarray)\n",
        "\n",
        "# CometNet class: Manages the neural network\n",
        "class CometNet:\n",
        "    def __init__(self, network, X, y, epochs=10, learning_rate=0.1, input_size=None):\n",
        "        \"\"\"\n",
        "        Initialize the CometNet class for training and predicting with a neural network.\n",
        "\n",
        "        Parameters:\n",
        "        network (list): List of layers in the network (each layer is an instance of a Layer subclass).\n",
        "        x_train (list): Training input data (list of np.ndarray).\n",
        "        y_train (list): Training target data (list of np.ndarray).\n",
        "        epochs (int): Number of training epochs (default: 1000).\n",
        "        learning_rate (float): Learning rate for updating the parameters (default: 0.01).\n",
        "        \"\"\"\n",
        "        self.network = network\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.input_size = input_size # Store the input size\n",
        "\n",
        "        # Initialize weights and biases for each layer if input_size is provided\n",
        "        if self.input_size:\n",
        "            for layer in self.network:\n",
        "                if isinstance(layer, Dense):\n",
        "                    layer.updater(self.input_size)\n",
        "                    self.input_size = layer.output_size # Update input size for the next layer\n",
        "\n",
        "    def predict(self, input):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the network to make predictions.\n",
        "\n",
        "        Applies the forward pass through each layer in the network to generate predictions\n",
        "        from the input data.\n",
        "\n",
        "        Parameters:\n",
        "        input (np.ndarray): Input data to the network (shape: [input_size, 1]).\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Output predictions from the network after passing through all layers (shape: [output_size, 1]).\n",
        "        \"\"\"\n",
        "        output = input  # Start with the input data (type: np.ndarray)\n",
        "        for layer in self.network:  # Pass input through each layer in the network\n",
        "            output = layer.forward(output)  # Apply each layer's forward pass (type: np.ndarray)\n",
        "        return output  # Return final output (type: np.ndarray)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train the neural network using the provided training data.\n",
        "\n",
        "        Iterates over the specified number of epochs, performing forward and backward passes\n",
        "        through the network for each training example. Updates the parameters of the network\n",
        "        based on the computed gradients.\n",
        "\n",
        "        This method prints the progress of training and updates the network parameters after\n",
        "        each epoch.\n",
        "        \"\"\"\n",
        "        for epoch in range(self.epochs):  # Iterate through each epoch (type: int)\n",
        "            error = 0  # Initialize error for this epoch (type: float)\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs} started.\")  # Debugging print statement\n",
        "            for x, y in tqdm(zip(self.X, self.y), total=len(self.X), desc=f\"Epoch {epoch + 1}\"):  # Iterate through training data with progress bar\n",
        "                # Ensure x is a 2D array\n",
        "                x = x.reshape(-1, 1) if x.ndim == 1 else x\n",
        "\n",
        "                # Ensure y is a 2D array with shape (1, 1)\n",
        "                y = np.array([[y]], dtype=float) if np.isscalar(y) else y.reshape(1, -1)\n",
        "\n",
        "                output = self.predict(x)  # Compute network output for the current input (type: np.ndarray)\n",
        "                error += Loss.mse(y, output)  # Calculate error using the Mean Squared Error loss function (type: float)\n",
        "                output_gradient = Loss.mse_derivative(y, output)  # Compute gradient of the loss with respect to the output (type: np.ndarray)\n",
        "                for layer in reversed(self.network):  # Update parameters starting from the last layer\n",
        "                    output_gradient = layer.backward(output_gradient, self.learning_rate)  # Perform backward pass and update parameters (type: np.ndarray)\n",
        "            error /= len(self.X)  # Compute average error over all training examples (type: float)\n",
        "            print(f\"Epoch {epoch + 1} completed with average error: {error:.6f}\")  # Print average error for the epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cyj43EH8Pi6J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.utils import resample\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"\n",
        "    A class for preprocessing network traffic data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, datetime_col='Timestamp', label_col='Label', random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize the DataPreprocessor.\n",
        "\n",
        "        Args:\n",
        "            datetime_col (str): Name of the datetime column.\n",
        "            label_col (str): Name of the label column.\n",
        "            random_state (int): Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        self.datetime_col = datetime_col\n",
        "        self.label_col = label_col\n",
        "        self.random_state = random_state\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "    def read_and_combine_data(self, file_paths):\n",
        "        \"\"\"\n",
        "        Read CSV files and combine them into a single DataFrame.\n",
        "\n",
        "        Args:\n",
        "            file_paths (list): List of paths to the CSV files.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Combined DataFrame.\n",
        "        \"\"\"\n",
        "        df_list = [pd.read_csv(file_path, encoding='latin1') for file_path in file_paths]\n",
        "        df = pd.concat(df_list).reset_index(drop=True)\n",
        "        df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
        "        return df\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        \"\"\"\n",
        "        Preprocess the data by encoding labels, handling timestamps, and dropping unnecessary columns.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Preprocessed DataFrame.\n",
        "        \"\"\"\n",
        "        # Encode labels\n",
        "        df[self.label_col] = self.label_encoder.fit_transform(df[self.label_col])\n",
        "        df[self.label_col] = df[self.label_col].apply(lambda x: 0 if x == 0 else 1)\n",
        "\n",
        "        # Handle timestamp\n",
        "        if self.datetime_col in df.columns:\n",
        "            df[self.datetime_col] = pd.to_datetime(df[self.datetime_col], errors='coerce')\n",
        "            df.dropna(subset=[self.datetime_col], inplace=True)\n",
        "            df['minutes_from_midnight'] = (df[self.datetime_col].dt.hour * 60 +\n",
        "                                           df[self.datetime_col].dt.minute +\n",
        "                                           df[self.datetime_col].dt.second / 60 +\n",
        "                                           df[self.datetime_col].dt.microsecond / 60000000)\n",
        "            df.drop(columns=[self.datetime_col], inplace=True)\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        columns_to_drop = ['Flow_ID', 'Source_IP', 'Destination_IP']\n",
        "        df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def resample_data(self, df, proportions):\n",
        "        \"\"\"\n",
        "        Resample the data to achieve desired class proportions.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input DataFrame.\n",
        "            proportions (list): Desired proportions for each label.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Resampled DataFrame.\n",
        "        \"\"\"\n",
        "        df_majority = df[df[self.label_col] == 0]\n",
        "        df_minority = df[df[self.label_col] == 1]\n",
        "\n",
        "        n_samples_majority = int(len(df) * proportions[0])\n",
        "        n_samples_minority = int(len(df) * proportions[1])\n",
        "\n",
        "        df_majority_resampled = resample(df_majority, replace=False, n_samples=n_samples_majority, random_state=self.random_state)\n",
        "        df_minority_resampled = resample(df_minority, replace=True, n_samples=n_samples_minority, random_state=self.random_state)\n",
        "\n",
        "        return pd.concat([df_majority_resampled, df_minority_resampled])\n",
        "\n",
        "    def select_features(self, df):\n",
        "        \"\"\"\n",
        "        Select features based on a predefined list.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with selected features.\n",
        "        \"\"\"\n",
        "        selected_features = [\n",
        "            'Source_Port', 'Destination_Port', 'Protocol', 'Flow_Duration', 'Fwd_Packet_Length_Max',\n",
        "            'Fwd_Packet_Length_Min', 'Fwd_Packet_Length_Mean', 'Fwd_Packet_Length_Std',\n",
        "            'Bwd_Packet_Length_Max', 'Bwd_Packet_Length_Min', 'Bwd_Packet_Length_Mean',\n",
        "            'Bwd_Packet_Length_Std', 'Flow_IAT_Mean', 'Flow_IAT_Std', 'Flow_IAT_Max',\n",
        "            'Fwd_IAT_Total', 'Fwd_IAT_Mean', 'Fwd_IAT_Std', 'Fwd_IAT_Max', 'Bwd_IAT_Std',\n",
        "            'Bwd_IAT_Max', 'Fwd_PSH_Flags', 'Bwd_Packets/s', 'Min_Packet_Length',\n",
        "            'Max_Packet_Length', 'Packet_Length_Mean', 'Packet_Length_Std',\n",
        "            'Packet_Length_Variance', 'FIN_Flag_Count', 'SYN_Flag_Count', 'ACK_Flag_Count',\n",
        "            'URG_Flag_Count', 'Down/Up_Ratio', 'Average_Packet_Size', 'Avg_Fwd_Segment_Size',\n",
        "            'Avg_Bwd_Segment_Size', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "            'Idle_Mean', 'Idle_Max', 'Idle_Min', 'minutes_from_midnight'\n",
        "        ]\n",
        "        return df[selected_features + [self.label_col]]\n",
        "\n",
        "    def scale_and_impute(self, X_train, X_test):\n",
        "        \"\"\"\n",
        "        Scale features and impute missing values.\n",
        "\n",
        "        Args:\n",
        "            X_train (np.array): Training feature set.\n",
        "            X_test (np.array): Test feature set.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Scaled and imputed training and test sets.\n",
        "        \"\"\"\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        X_train_imputed = self.imputer.fit_transform(X_train_scaled)\n",
        "        X_test_imputed = self.imputer.transform(X_test_scaled)\n",
        "\n",
        "        return X_train_imputed, X_test_imputed\n",
        "\n",
        "    def process_data(self, file_paths, proportions, verbose=False):\n",
        "        \"\"\"\n",
        "        Process the data through all preprocessing steps.\n",
        "\n",
        "        Args:\n",
        "            file_paths (list): List of paths to the CSV files.\n",
        "            proportions (list): Desired proportions for each label.\n",
        "            verbose (bool): Whether to print additional information.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Processed training and test sets (X_train, X_test, y_train, y_test).\n",
        "        \"\"\"\n",
        "        df = self.read_and_combine_data(file_paths)\n",
        "        df = self.preprocess_data(df)\n",
        "        df_resampled = self.resample_data(df, proportions)\n",
        "        df_filtered = self.select_features(df_resampled)\n",
        "\n",
        "        y = df_filtered[self.label_col]\n",
        "        X = df_filtered.drop(columns=[self.label_col])\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=self.random_state)\n",
        "        X_train_imputed, X_test_imputed = self.scale_and_impute(X_train, X_test)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Final shapes:\")\n",
        "            print(\"X_train shape:\", X_train_imputed.shape)\n",
        "            print(\"X_test shape:\", X_test_imputed.shape)\n",
        "            print(\"y_train shape:\", y_train.shape)\n",
        "            print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "        return X_train_imputed, X_test_imputed, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr5jSMpuPi6K",
        "outputId": "6e79845c-06ad-4e43-c9b3-08135b2144ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing training data:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6461e323bdee>:39: DtypeWarning: Columns (0,1,3,6,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_list = [pd.read_csv(file_path, encoding='latin1') for file_path in file_paths]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing test data:\n",
            "Final shapes:\n",
            "X_train shape: (554161, 42)\n",
            "X_test shape: (138541, 42)\n",
            "y_train shape: (554161,)\n",
            "y_test shape: (138541,)\n"
          ]
        }
      ],
      "source": [
        "# Define file paths\n",
        "train_file_paths = [\n",
        "    r'/content/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
        "    r'/content/Monday-WorkingHours.pcap_ISCX.csv',\n",
        "    r'/content/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv'\n",
        "]\n",
        "test_file_paths = [\n",
        "    r'/content/Wednesday-workingHours.pcap_ISCX.csv'\n",
        "]\n",
        "\n",
        "# Define desired proportions for each label\n",
        "proportions = [0.4, 0.6]\n",
        "\n",
        "# Initialize DataPreprocessor\n",
        "preprocessor = DataPreprocessor(datetime_col='Timestamp', label_col='Label', random_state=42)\n",
        "\n",
        "# Process training data\n",
        "print(\"Processing training data:\")\n",
        "X_train, X_test, y_train, y_test = preprocessor.process_data(train_file_paths, proportions, verbose=False)\n",
        "\n",
        "# Process test data\n",
        "print(\"\\nProcessing test data:\")\n",
        "X_train_test, X_test_test, y_train_test, y_test_test = preprocessor.process_data(test_file_paths, proportions, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXTaBf21Pi6L",
        "outputId": "759b4197-6f32-49a7-9d15-c6386a2a59cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "X_train[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "b399yGSrPi6L",
        "outputId": "90ae63d4-3d5c-488a-9721-dcedffff0bb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 started.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   0%|          | 0/316888 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-0e3285e3dd73>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcomet_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCometNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pass input size to the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcomet_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-6479e9202a48>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute network output for the current input (type: np.ndarray)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Calculate error using the Mean Squared Error loss function (type: float)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0moutput_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute gradient of the loss with respect to the output (type: np.ndarray)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-6479e9202a48>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m  \u001b[0;31m# Start with the input data (type: np.ndarray)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Pass input through each layer in the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply each layer's forward pass (type: np.ndarray)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m  \u001b[0;31m# Return final output (type: np.ndarray)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-6479e9202a48>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
          ]
        }
      ],
      "source": [
        "X_train = np.array(X_train)  # Ensure X_train is a numpy array\n",
        "y_train = np.array(y_train)  # Ensure y_train is a numpy array\n",
        "# Define the network\n",
        "network = [\n",
        "    Dense(100),\n",
        "    Sigmoid(),\n",
        "    Dense(200),\n",
        "    Sigmoid(),\n",
        "    Dense(1),\n",
        "    Sigmoid()\n",
        "]\n",
        "\n",
        "comet_net = CometNet(network, X_train, y_train, epochs=2, learning_rate=0.1, input_size=X_train.shape[1]) # Pass input size to the network\n",
        "comet_net.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array(X_test)\n",
        "# test the model\n",
        "y_pred = comet_net.predict(X_test)\n",
        "print(y_pred)\n",
        "\n",
        "# check confution matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Y-4NM1fQWwYn",
        "outputId": "ddc52df1-8b8b-4fc8-ed09-04beb7414852"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (100,42) and (79222,42) not aligned: 42 (dim 1) != 79222 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1d2bfd46a349>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# test the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomet_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-eadac4c6d838>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m  \u001b[0;31m# Start with the input data (type: np.ndarray)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Pass input through each layer in the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply each layer's forward pass (type: np.ndarray)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m  \u001b[0;31m# Return final output (type: np.ndarray)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-eadac4c6d838>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (100,42) and (79222,42) not aligned: 42 (dim 1) != 79222 (dim 0)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}