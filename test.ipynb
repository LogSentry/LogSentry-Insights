{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import geoip2.database\n",
    "\n",
    "def get_country(ip, reader):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        return response.country.iso_code\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def calculate_entropy(payload):\n",
    "    return entropy(pd.Series(list(str(payload))).value_counts())\n",
    "\n",
    "def process_network_csv(csv_path, db_path, output_path=None):\n",
    "    # Read the CSV file and remove leading/trailing spaces from column names\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Print the columns of the input CSV to verify the column names\n",
    "    print(\"Columns in the input CSV file after stripping spaces:\", df.columns)\n",
    "\n",
    "    # Check for the required columns and handle missing ones\n",
    "    required_columns = [\n",
    "        'Flow Packets/s', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'SYN Flag Count', 'Protocol', \n",
    "        'Total Fwd Packets', 'Total Backward Packets', 'Flow ID', 'Source IP', 'Flow Duration', 'Flow IAT Mean', \n",
    "        'Timestamp', 'Fwd Packet Length Std', 'Bwd Packet Length Std', 'Packet Length Variance', 'Fwd IAT Std', \n",
    "        'Bwd IAT Std', 'Flow IAT Std', 'Active Mean', 'Active Std', 'Idle Mean', 'Idle Std', \n",
    "        'Subflow Fwd Packets', 'Subflow Bwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes', \n",
    "        'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'Fwd PSH Flags', 'Bwd PSH Flags', \n",
    "        'Fwd URG Flags', 'Bwd URG Flags'\n",
    "    ]\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise KeyError(f\"The following required columns are missing in the input CSV file: {', '.join(missing_columns)}\")\n",
    "\n",
    "    # 1. Traffic Volume Metrics\n",
    "    df['Requests_per_Second'] = df['Flow Packets/s']\n",
    "    df['Total_Bandwidth_Consumption'] = df['Total Length of Fwd Packets'] + df['Total Length of Bwd Packets']\n",
    "    df['Packet_Rate'] = df['Flow Packets/s']\n",
    "\n",
    "    # 2. Protocol-Specific Features\n",
    "    df['TCP_SYN_Packet_Count'] = df['SYN Flag Count']\n",
    "    df['UDP_Packet_Count'] = df[df['Protocol'] == 17]['Total Fwd Packets'] + df[df['Protocol'] == 17]['Total Backward Packets']\n",
    "    df['ICMP_Packet_Count'] = df[df['Protocol'] == 1]['Total Fwd Packets'] + df[df['Protocol'] == 1]['Total Backward Packets']\n",
    "\n",
    "    # 3. IP Address Diversity\n",
    "    df['Unique_Source_IPs'] = df.groupby('Flow ID')['Source IP'].transform('nunique')\n",
    "\n",
    "    # 4. Time-Based Features\n",
    "    df['Connection_Duration'] = df['Flow Duration']\n",
    "    df['Inter_Arrival_Time'] = df['Flow IAT Mean']\n",
    "    df['Time_Pattern'] = pd.to_datetime(df['Timestamp']).dt.hour\n",
    "\n",
    "    # 5. Payload Analysis\n",
    "    df['Payload_Size'] = df['Total Length of Fwd Packets'] + df['Total Length of Bwd Packets']\n",
    "    df['Payload_Entropy'] = (df['Total Length of Fwd Packets'] + df['Total Length of Bwd Packets']).apply(calculate_entropy)\n",
    "\n",
    "    # 6. Flag Analysis\n",
    "    flag_columns = ['FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count']\n",
    "    df['Total_Flags'] = df[flag_columns].sum(axis=1)\n",
    "    df['Flag_Distribution'] = df[flag_columns].apply(lambda row: ','.join(f\"{col.split()[0]}:{val}\" for col, val in row.items()), axis=1)\n",
    "\n",
    "    # 7. Packet Length Statistics\n",
    "    df['Fwd_Packet_Length_Std'] = df['Fwd Packet Length Std']\n",
    "    df['Bwd_Packet_Length_Std'] = df['Bwd Packet Length Std']\n",
    "    df['Packet_Length_Variance'] = df['Packet Length Variance']\n",
    "\n",
    "    # 8. Flow Inter-arrival Time Statistics\n",
    "    df['Fwd_IAT_Std'] = df['Fwd IAT Std']\n",
    "    df['Bwd_IAT_Std'] = df['Bwd IAT Std']\n",
    "    df['Flow_IAT_Std'] = df['Flow IAT Std']\n",
    "\n",
    "    # 9. Active and Idle Time\n",
    "    df['Active_Time_Mean'] = df['Active Mean']\n",
    "    df['Active_Time_Std'] = df['Active Std']\n",
    "    df['Idle_Time_Mean'] = df['Idle Mean']\n",
    "    df['Idle_Time_Std'] = df['Idle Std']\n",
    "\n",
    "    # 10. Subflow Statistics\n",
    "    df['Subflow_Fwd_Packets'] = df['Subflow Fwd Packets']\n",
    "    df['Subflow_Bwd_Packets'] = df['Subflow Bwd Packets']\n",
    "    df['Subflow_Fwd_Bytes'] = df['Subflow Fwd Bytes']\n",
    "    df['Subflow_Bwd_Bytes'] = df['Subflow Bwd Bytes']\n",
    "\n",
    "    # 11. Window Size Statistics\n",
    "    df['Init_Win_Bytes_Forward'] = df['Init_Win_bytes_forward']\n",
    "    df['Init_Win_Bytes_Backward'] = df['Init_Win_bytes_backward']\n",
    "\n",
    "    # 12. Packet Count Ratio\n",
    "    df['Packet_Count_Ratio'] = df['Total Fwd Packets'] / (df['Total Backward Packets'] + 1)  # Adding 1 to avoid division by zero\n",
    "\n",
    "    # 13. Byte Count Ratio\n",
    "    df['Byte_Count_Ratio'] = df['Total Length of Fwd Packets'] / (df['Total Length of Bwd Packets'] + 1)  # Adding 1 to avoid division by zero\n",
    "\n",
    "    # 14. PSH and URG Flag Ratios\n",
    "    total_packets = df['Total Fwd Packets'] + df['Total Backward Packets']\n",
    "    df['PSH_Flag_Ratio'] = (df['Fwd PSH Flags'] + df['Bwd PSH Flags']) / total_packets\n",
    "    df['URG_Flag_Ratio'] = (df['Fwd URG Flags'] + df['Bwd URG Flags']) / total_packets\n",
    "\n",
    "    # 15. Average Packet Size\n",
    "    df['Avg_Packet_Size'] = (df['Total Length of Fwd Packets'] + df['Total Length of Bwd Packets']) / total_packets\n",
    "\n",
    "    # Multithreading for geolocation using local database\n",
    "    reader = geoip2.database.Reader(db_path)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_ip = {executor.submit(get_country, ip, reader): ip for ip in df['Source IP']}\n",
    "        for future in as_completed(future_to_ip):\n",
    "            ip = future_to_ip[future]\n",
    "            try:\n",
    "                country = future.result()\n",
    "                df.loc[df['Source IP'] == ip, 'Country'] = country\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving country for IP {ip}: {e}\")\n",
    "\n",
    "    # Save the enhanced DataFrame to a CSV file if an output path is provided\n",
    "    if output_path:\n",
    "        df.to_csv(output_path, index=False)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x96 in position 22398: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLogicInsights\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mworks\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Process the CSV and get the enhanced DataFrame\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df_enhanced \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_network_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Print a summary of the processed data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_enhanced\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m, in \u001b[0;36mprocess_network_csv\u001b[1;34m(csv_path, db_path, output_path)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_network_csv\u001b[39m(csv_path, db_path, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Read the CSV file and remove leading/trailing spaces from column names\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Print the columns of the input CSV to verify the column names\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kani2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kani2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kani2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\kani2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x96 in position 22398: invalid start byte"
     ]
    }
   ],
   "source": [
    "csv_path = r\"C:\\Users\\kani2\\Downloads\\Datasets\\GeneratedLabelledFlows (1)\\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\"\n",
    "db_path = r'GeoLite2-City_20240723\\GeoLite2-City_20240723\\GeoLite2-City.mmdb'\n",
    "output_path = r'E:\\LogicInsights\\works\\output.csv'\n",
    "\n",
    "# Process the CSV and get the enhanced DataFrame\n",
    "df_enhanced = process_network_csv(csv_path, db_path, output_path)\n",
    "\n",
    "# Print a summary of the processed data\n",
    "print(df_enhanced.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted C:\\Users\\kani2\\Downloads\\GeoLite2-City_20240723.tar.gz to C:\\Users\\kani2\\Downloads\\GeoLite2-City_20240723\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "def extract_tar_gz(file_path, output_dir):\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        tar.extractall(output_dir)\n",
    "    print(f\"Successfully extracted {file_path} to {output_dir}\")\n",
    "\n",
    "extract_tar_gz(r\"C:\\Users\\kani2\\Downloads\\GeoLite2-City_20240723.tar.gz\", r\"C:\\Users\\kani2\\Downloads\\GeoLite2-City_20240723\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
