{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uR3rx_EPi6E"
      },
      "source": [
        "# Layer by Layer approach for bulding a MLP (Multi-Layer Perseptron) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "aqb96IjYPi6H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        pass\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        pass\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, output_size):\n",
        "        \"\"\"\n",
        "        Initialize the Dense layer with given output size.\n",
        "\n",
        "        Parameters:\n",
        "        output_size (int): Number of neurons in this layer.\n",
        "        \"\"\"\n",
        "        self.output_size = output_size\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def initialize(self, input_size):\n",
        "        \"\"\"\n",
        "        Initialize weights and biases based on the input size.\n",
        "\n",
        "        Parameters:\n",
        "        input_size (int): Size of the input to this layer.\n",
        "        \"\"\"\n",
        "        self.weights = np.random.randn(self.output_size, input_size) \n",
        "        self.bias = np.zeros((self.output_size, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform the forward pass through the dense layer.\n",
        "\n",
        "        Parameters:\n",
        "        input (np.ndarray): Input data to the layer (shape: [input_size, 1]).\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Output data from the layer (shape: [output_size, 1]).\n",
        "        \"\"\"\n",
        "        self.input = input\n",
        "        if self.weights is None:\n",
        "            self.initialize(input.shape[0])\n",
        "        self.output = np.dot(self.weights, self.input) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        \"\"\"\n",
        "        Perform the backward pass through the dense layer and update its parameters.\n",
        "\n",
        "        Parameters:\n",
        "        output_gradient (np.ndarray): Gradient of the loss function with respect to the layer's output.\n",
        "        learning_rate (float): Learning rate for updating the weights and biases.\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Gradient of the loss function with respect to the input data.\n",
        "        \"\"\"\n",
        "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.bias -= learning_rate * output_gradient\n",
        "        return np.dot(self.weights.T, output_gradient)\n",
        "\n",
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        \"\"\"\n",
        "        Initialize the Activation layer with a specific activation function and its derivative.\n",
        "\n",
        "        Parameters:\n",
        "        activation (callable): Activation function to be applied.\n",
        "        activation_prime (callable): Derivative of the activation function.\n",
        "        \"\"\"\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform the forward pass through the activation function.\n",
        "\n",
        "        Parameters:\n",
        "        input (np.ndarray): Input data to the activation function.\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Output data after applying the activation function.\n",
        "        \"\"\"\n",
        "        self.input = input\n",
        "        return self.activation(self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        \"\"\"\n",
        "        Perform the backward pass through the activation function.\n",
        "\n",
        "        Parameters:\n",
        "        output_gradient (np.ndarray): Gradient of the loss function with respect to the activation function's output.\n",
        "        learning_rate (float): Learning rate (not used in this method, but included for consistency).\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Gradient of the loss function with respect to the input data.\n",
        "        \"\"\"\n",
        "        return np.multiply(output_gradient, self.activation_prime(self.input))\n",
        "\n",
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the Sigmoid activation function and its derivative.\n",
        "        \"\"\"\n",
        "        def sigmoid(x):\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "\n",
        "        def sigmoid_prime(x):\n",
        "            s = sigmoid(x)\n",
        "            return s * (1 - s)\n",
        "\n",
        "        super().__init__(sigmoid, sigmoid_prime)\n",
        "\n",
        "class ReLU(Activation):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the Sigmoid activation function and its derivative.\n",
        "        \"\"\"\n",
        "        def ReLU(x):\n",
        "            return np.maximum(0, x)\n",
        "\n",
        "        def ReLU_prime(x):\n",
        "            return 1 if x > 0 else 0\n",
        "\n",
        "        super().__init__(ReLU, ReLU_prime)\n",
        "\n",
        "\n",
        "class Loss:\n",
        "    @staticmethod\n",
        "    def mse(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate the Mean Squared Error (MSE) loss.\n",
        "\n",
        "        Parameters:\n",
        "        y_true (np.ndarray): True label.\n",
        "        y_pred (np.ndarray): Predicted label.\n",
        "\n",
        "        Returns:\n",
        "        float: MSE loss value.\n",
        "        \"\"\"\n",
        "        return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "    @staticmethod\n",
        "    def mse_derivative(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate the derivative of the Mean Squared Error (MSE) loss function.\n",
        "\n",
        "        Parameters:\n",
        "        y_true (np.ndarray): True label.\n",
        "        y_pred (np.ndarray): Predicted label.\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Gradient of MSE loss with respect to prediction.\n",
        "        \"\"\"\n",
        "        return 2 * (y_pred - y_true) / np.size(y_true)\n",
        "\n",
        "class CometNet:\n",
        "    def __init__(self, layers, input_size):\n",
        "        \"\"\"\n",
        "        Initialize the CometNet class for training and predicting with a neural network.\n",
        "\n",
        "        Parameters:\n",
        "        layers (list): List of layers in the network (each layer is an instance of a Layer subclass).\n",
        "        input_size (int): Size of the input features.\n",
        "        \"\"\"\n",
        "        self.layers = layers\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def predict(self, input):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the network to make predictions.\n",
        "\n",
        "        Parameters:\n",
        "        input (np.ndarray): Input data to the network (shape: [input_size, 1]).\n",
        "\n",
        "        Returns:\n",
        "        np.ndarray: Output prediction from the network (shape: [output_size, 1]).\n",
        "        \"\"\"\n",
        "        output = input\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "\n",
        "    def train(self, X, y, epochs=10, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        Train the neural network using the provided training data.\n",
        "\n",
        "        Parameters:\n",
        "        X (np.ndarray): Training input data.\n",
        "        y (np.ndarray): Training target data.\n",
        "        epochs (int): Number of training epochs (default: 10).\n",
        "        learning_rate (float): Learning rate for updating the parameters (default: 0.1).\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            error = 0\n",
        "            for x, y_true in tqdm(zip(X, y), total=len(X), desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
        "                x = x.reshape(-1, 1)\n",
        "                y_true = np.array([[y_true]], dtype=float)\n",
        "                \n",
        "                output = self.predict(x)\n",
        "                error += Loss.mse(y_true, output)\n",
        "                \n",
        "                grad = Loss.mse_derivative(y_true, output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    grad = layer.backward(grad, learning_rate)\n",
        "            \n",
        "            error /= len(X)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} - Error: {error:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cyj43EH8Pi6J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.utils import resample\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"\n",
        "    A class for preprocessing network traffic data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, datetime_col='Timestamp', label_col='Label', random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize the DataPreprocessor.\n",
        "\n",
        "        Args:\n",
        "            datetime_col (str): Name of the datetime column.\n",
        "            label_col (str): Name of the label column.\n",
        "            random_state (int): Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        self.datetime_col = datetime_col\n",
        "        self.label_col = label_col\n",
        "        self.random_state = random_state\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "    def read_and_combine_data(self, file_paths):\n",
        "        \"\"\"\n",
        "        Read CSV files and combine them into a single DataFrame.\n",
        "\n",
        "        Args:\n",
        "            file_paths (list): List of paths to the CSV files.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Combined DataFrame.\n",
        "        \"\"\"\n",
        "        df_list = [pd.read_csv(file_path, encoding='latin1') for file_path in file_paths]\n",
        "        df = pd.concat(df_list).reset_index(drop=True)\n",
        "        df.columns = df.columns.str.strip().str.replace(' ', '_')\n",
        "        return df\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        \"\"\"\n",
        "        Preprocess the data by encoding labels, handling timestamps, and dropping unnecessary columns.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Preprocessed DataFrame.\n",
        "        \"\"\"\n",
        "        # Encode labels\n",
        "        df[self.label_col] = self.label_encoder.fit_transform(df[self.label_col])\n",
        "        df[self.label_col] = df[self.label_col].apply(lambda x: 0 if x == 0 else 1)\n",
        "\n",
        "        # Handle timestamp\n",
        "        if self.datetime_col in df.columns:\n",
        "            df[self.datetime_col] = pd.to_datetime(df[self.datetime_col], errors='coerce')\n",
        "            df.dropna(subset=[self.datetime_col], inplace=True)\n",
        "            df['minutes_from_midnight'] = (df[self.datetime_col].dt.hour * 60 +\n",
        "                                           df[self.datetime_col].dt.minute +\n",
        "                                           df[self.datetime_col].dt.second / 60 +\n",
        "                                           df[self.datetime_col].dt.microsecond / 60000000)\n",
        "            df.drop(columns=[self.datetime_col], inplace=True)\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        columns_to_drop = ['Flow_ID', 'Source_IP', 'Destination_IP']\n",
        "        df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def resample_data(self, df, proportions):\n",
        "        \"\"\"\n",
        "        Resample the data to achieve desired class proportions.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input DataFrame.\n",
        "            proportions (list): Desired proportions for each label.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Resampled DataFrame.\n",
        "        \"\"\"\n",
        "        df_majority = df[df[self.label_col] == 0]\n",
        "        df_minority = df[df[self.label_col] == 1]\n",
        "\n",
        "        n_samples_majority = int(len(df) * proportions[0])\n",
        "        n_samples_minority = int(len(df) * proportions[1])\n",
        "\n",
        "        df_majority_resampled = resample(df_majority, replace=False, n_samples=n_samples_majority, random_state=self.random_state)\n",
        "        df_minority_resampled = resample(df_minority, replace=True, n_samples=n_samples_minority, random_state=self.random_state)\n",
        "\n",
        "        return pd.concat([df_majority_resampled, df_minority_resampled])\n",
        "\n",
        "    def select_features(self, df):\n",
        "        \"\"\"\n",
        "        Select features based on a predefined list.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with selected features.\n",
        "        \"\"\"\n",
        "        selected_features = [\n",
        "            'Source_Port', 'Destination_Port', 'Protocol', 'Flow_Duration', 'Fwd_Packet_Length_Max',\n",
        "            'Fwd_Packet_Length_Min', 'Fwd_Packet_Length_Mean', 'Fwd_Packet_Length_Std',\n",
        "            'Bwd_Packet_Length_Max', 'Bwd_Packet_Length_Min', 'Bwd_Packet_Length_Mean',\n",
        "            'Bwd_Packet_Length_Std', 'Flow_IAT_Mean', 'Flow_IAT_Std', 'Flow_IAT_Max',\n",
        "            'Fwd_IAT_Total', 'Fwd_IAT_Mean', 'Fwd_IAT_Std', 'Fwd_IAT_Max', 'Bwd_IAT_Std',\n",
        "            'Bwd_IAT_Max', 'Fwd_PSH_Flags', 'Bwd_Packets/s', 'Min_Packet_Length',\n",
        "            'Max_Packet_Length', 'Packet_Length_Mean', 'Packet_Length_Std',\n",
        "            'Packet_Length_Variance', 'FIN_Flag_Count', 'SYN_Flag_Count', 'ACK_Flag_Count',\n",
        "            'URG_Flag_Count', 'Down/Up_Ratio', 'Average_Packet_Size', 'Avg_Fwd_Segment_Size',\n",
        "            'Avg_Bwd_Segment_Size', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "            'Idle_Mean', 'Idle_Max', 'Idle_Min', 'minutes_from_midnight'\n",
        "        ]\n",
        "        return df[selected_features + [self.label_col]]\n",
        "\n",
        "    def scale_and_impute(self, X_train, X_test):\n",
        "        \"\"\"\n",
        "        Scale features and impute missing values.\n",
        "\n",
        "        Args:\n",
        "            X_train (np.array): Training feature set.\n",
        "            X_test (np.array): Test feature set.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Scaled and imputed training and test sets.\n",
        "        \"\"\"\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        X_train_imputed = self.imputer.fit_transform(X_train_scaled)\n",
        "        X_test_imputed = self.imputer.transform(X_test_scaled)\n",
        "\n",
        "        return X_train_imputed, X_test_imputed\n",
        "\n",
        "    def process_data(self, file_paths, proportions, verbose=False):\n",
        "        \"\"\"\n",
        "        Process the data through all preprocessing steps.\n",
        "\n",
        "        Args:\n",
        "            file_paths (list): List of paths to the CSV files.\n",
        "            proportions (list): Desired proportions for each label.\n",
        "            verbose (bool): Whether to print additional information.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Processed training and test sets (X_train, X_test, y_train, y_test).\n",
        "        \"\"\"\n",
        "        df = self.read_and_combine_data(file_paths)\n",
        "        df = self.preprocess_data(df)\n",
        "        df_resampled = self.resample_data(df, proportions)\n",
        "        df_filtered = self.select_features(df_resampled)\n",
        "\n",
        "        y = df_filtered[self.label_col]\n",
        "        X = df_filtered.drop(columns=[self.label_col])\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=self.random_state)\n",
        "        X_train_imputed, X_test_imputed = self.scale_and_impute(X_train, X_test)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Final shapes:\")\n",
        "            print(\"X_train shape:\", X_train_imputed.shape)\n",
        "            print(\"X_test shape:\", X_test_imputed.shape)\n",
        "            print(\"y_train shape:\", y_train.shape)\n",
        "            print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "        return X_train_imputed, X_test_imputed, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr5jSMpuPi6K",
        "outputId": "6e79845c-06ad-4e43-c9b3-08135b2144ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing training data:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kani2\\AppData\\Local\\Temp\\ipykernel_12160\\2644717438.py:39: DtypeWarning: Columns (0,1,3,6,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_list = [pd.read_csv(file_path, encoding='latin1') for file_path in file_paths]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing test data:\n",
            "Final shapes:\n",
            "X_train shape: (554161, 42)\n",
            "X_test shape: (138541, 42)\n",
            "y_train shape: (554161,)\n",
            "y_test shape: (138541,)\n"
          ]
        }
      ],
      "source": [
        "# Define file paths\n",
        "train_file_paths = [\n",
        "    r'GeneratedLabelledFlows (1)\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
        "    r'GeneratedLabelledFlows (1)\\Monday-WorkingHours.pcap_ISCX.csv',\n",
        "    r'GeneratedLabelledFlows (1)\\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv'\n",
        "]\n",
        "test_file_paths = [\n",
        "    r'GeneratedLabelledFlows (1)\\Wednesday-workingHours.pcap_ISCX.csv'\n",
        "]\n",
        "\n",
        "# Define desired proportions for each label\n",
        "proportions = [0.5, 0.5]\n",
        "\n",
        "# Initialize DataPreprocessor\n",
        "preprocessor = DataPreprocessor(datetime_col='Timestamp', label_col='Label', random_state=42)\n",
        "\n",
        "# Process training data\n",
        "print(\"Processing training data:\")\n",
        "X_train, X_test, y_train, y_test = preprocessor.process_data(train_file_paths, proportions, verbose=False)\n",
        "\n",
        "# Process test data\n",
        "print(\"\\nProcessing test data:\")\n",
        "X_train_test, X_test_test, y_train_test, y_test_test = preprocessor.process_data(test_file_paths, proportions, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXTaBf21Pi6L",
        "outputId": "759b4197-6f32-49a7-9d15-c6386a2a59cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(42,)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "b399yGSrPi6L",
        "outputId": "90ae63d4-3d5c-488a-9721-dcedffff0bb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/8: 100%|██████████| 316888/316888 [11:17<00:00, 468.01it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8 - Error: 0.038216\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/8:  38%|███▊      | 118974/316888 [03:30<05:50, 565.28it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[53], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m CometNet(network, input_size)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train the network\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[49], line 193\u001b[0m, in \u001b[0;36mCometNet.train\u001b[1;34m(self, X, y, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    190\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    191\u001b[0m y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[y_true]], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m--> 193\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Loss\u001b[38;5;241m.\u001b[39mmse(y_true, output)\n\u001b[0;32m    196\u001b[0m grad \u001b[38;5;241m=\u001b[39m Loss\u001b[38;5;241m.\u001b[39mmse_derivative(y_true, output)\n",
            "Cell \u001b[1;32mIn[49], line 174\u001b[0m, in \u001b[0;36mCometNet.predict\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    172\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 174\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "Cell \u001b[1;32mIn[49], line 50\u001b[0m, in \u001b[0;36mDense.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "X_train = np.array(X_train)  # Ensure X_train is a numpy array\n",
        "y_train = np.array(y_train)  # Ensure y_train is a numpy array\n",
        "# Define the network architecture\n",
        "input_size = X_train[0].shape  # User-defined input size\n",
        "network = [\n",
        "    Dense(128),\n",
        "    ReLU(),\n",
        "    Dense(250),\n",
        "    Sigmoid(),\n",
        "    Dense(150),\n",
        "    Sigmoid(),\n",
        "    Dense(1),\n",
        "    Sigmoid()\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "model = CometNet(network, input_size)\n",
        "\n",
        "# Train the network\n",
        "model.train(X_train, y_train, epochs=8, learning_rate=0.01)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Y-4NM1fQWwYn",
        "outputId": "ddc52df1-8b8b-4fc8-ed09-04beb7414852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(79222, 1)\n",
            "(79222, 1)\n",
            "(79222, 1)\n",
            "[[35659  4049]\n",
            " [35694  3820]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.90      0.64     39708\n",
            "           1       0.49      0.10      0.16     39514\n",
            "\n",
            "    accuracy                           0.50     79222\n",
            "   macro avg       0.49      0.50      0.40     79222\n",
            "weighted avg       0.49      0.50      0.40     79222\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_test = X_test.reshape(input_size[0], -1)\n",
        "y_test = y_test.reshape(y_test.shape[0], -1)\n",
        "\n",
        "# test the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "y_pred_classes = y_pred_classes.T\n",
        "print(y_pred.T.shape)\n",
        "print(y_test.shape)\n",
        "print(y_pred_classes.shape)\n",
        "y_pred_classes = y_pred_classes.reshape(y_pred_classes.shape[0], -1)\n",
        "\n",
        "\n",
        "# # check confution matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "print(cm)\n",
        "\n",
        "#classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred_classes))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
