{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import geoip2.database\n",
    "\n",
    "def get_country(ip, reader):\n",
    "    try:\n",
    "        response = reader.city(ip)\n",
    "        return response.country.iso_code\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def calculate_entropy(payload):\n",
    "    return entropy(pd.Series(list(str(payload))).value_counts())\n",
    "\n",
    "def process_network_csv(csv_path, db_path, output_path=None, max_workers=20):\n",
    "    # Read the CSV file with latin1 encoding and remove leading/trailing spaces from column names\n",
    "    df = pd.read_csv(csv_path, encoding='latin1')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Print the columns of the input CSV to verify the column names\n",
    "    print(\"Columns in the input CSV file after stripping spaces:\", df.columns)\n",
    "\n",
    "    # Check for the required columns and handle missing ones\n",
    "    required_columns = [\n",
    "        'Flow Packets/s', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'SYN Flag Count', 'Protocol', \n",
    "        'Total Fwd Packets', 'Total Backward Packets', 'Flow ID', 'Source IP', 'Flow Duration', 'Flow IAT Mean', \n",
    "        'Timestamp', 'Fwd Packet Length Std', 'Bwd Packet Length Std', 'Packet Length Variance', 'Fwd IAT Std', \n",
    "        'Bwd IAT Std', 'Flow IAT Std', 'Active Mean', 'Active Std', 'Idle Mean', 'Idle Std', \n",
    "        'Subflow Fwd Packets', 'Subflow Bwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Bytes', \n",
    "        'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'Fwd PSH Flags', 'Bwd PSH Flags', \n",
    "        'Fwd URG Flags', 'Bwd URG Flags'\n",
    "    ]\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise KeyError(f\"The following required columns are missing in the input CSV file: {', '.join(missing_columns)}\")\n",
    "\n",
    "    # 1. Traffic Volume Metrics\n",
    "    df['Total_Bandwidth_Consumption'] = df['Total Length of Fwd Packets'] + df['Total Length of Bwd Packets']\n",
    "\n",
    "    # 2. Protocol-Specific Features\n",
    "    df['TCP_SYN_Packet_Count'] = df['SYN Flag Count']\n",
    "    df['UDP_Packet_Count'] = df[df['Protocol'] == 17]['Total Fwd Packets'] + df[df['Protocol'] == 17]['Total Backward Packets']\n",
    "    df['ICMP_Packet_Count'] = df[df['Protocol'] == 1]['Total Fwd Packets'] + df[df['Protocol'] == 1]['Total Backward Packets']\n",
    "\n",
    "    # 3. IP Address Diversity\n",
    "    df['Unique_Source_IPs'] = df.groupby('Flow ID')['Source IP'].transform('nunique')\n",
    "\n",
    "    # 4. Time-Based Features\n",
    "    df['Connection_Duration'] = df['Flow Duration']\n",
    "    df['Inter_Arrival_Time'] = df['Flow IAT Mean']\n",
    "    df['Hour_of_Day'] = pd.to_datetime(df['Timestamp'], errors='coerce').dt.hour\n",
    "\n",
    "    # 5. Payload Analysis\n",
    "    df['Payload_Size'] = df['Total Length of Fwd Packets'] + df['Total Length of Bwd Packets']\n",
    "    df['Payload_Entropy'] = df['Payload_Size'].apply(calculate_entropy)\n",
    "\n",
    "    # 6. Flag Analysis\n",
    "    flag_columns = ['FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count']\n",
    "    df['Total_Flags'] = df[flag_columns].sum(axis=1)\n",
    "    df['Flag_Distribution'] = df[flag_columns].apply(lambda row: ','.join(f\"{col.split()[0]}:{val}\" for col, val in row.items()), axis=1)\n",
    "\n",
    "    # 7. Packet Length Statistics\n",
    "    df['Fwd_Packet_Length_Std'] = df['Fwd Packet Length Std']\n",
    "    df['Bwd_Packet_Length_Std'] = df['Bwd Packet Length Std']\n",
    "    df['Packet_Length_Variance'] = df['Packet Length Variance']\n",
    "\n",
    "    # 8. Flow Inter-arrival Time Statistics\n",
    "    df['Fwd_IAT_Std'] = df['Fwd IAT Std']\n",
    "    df['Bwd_IAT_Std'] = df['Bwd IAT Std']\n",
    "    df['Flow_IAT_Std'] = df['Flow IAT Std']\n",
    "\n",
    "    # 9. Active and Idle Time\n",
    "    df['Active_Time_Mean'] = df['Active Mean']\n",
    "    df['Active_Time_Std'] = df['Active Std']\n",
    "    df['Idle_Time_Mean'] = df['Idle Mean']\n",
    "    df['Idle_Time_Std'] = df['Idle Std']\n",
    "\n",
    "    # 10. Subflow Statistics\n",
    "    df['Subflow_Fwd_Packets'] = df['Subflow Fwd Packets']\n",
    "    df['Subflow_Bwd_Packets'] = df['Subflow Bwd Packets']\n",
    "    df['Subflow_Fwd_Bytes'] = df['Subflow Fwd Bytes']\n",
    "    df['Subflow_Bwd_Bytes'] = df['Subflow Bwd Bytes']\n",
    "\n",
    "    # 11. Window Size Statistics\n",
    "    df['Init_Win_Bytes_Forward'] = df['Init_Win_bytes_forward']\n",
    "    df['Init_Win_Bytes_Backward'] = df['Init_Win_bytes_backward']\n",
    "\n",
    "    # 12. Packet Count Ratio\n",
    "    df['Packet_Count_Ratio'] = df['Total Fwd Packets'] / (df['Total Backward Packets'] + 1)  # Adding 1 to avoid division by zero\n",
    "\n",
    "    # 13. Byte Count Ratio\n",
    "    df['Byte_Count_Ratio'] = df['Total Length of Fwd Packets'] / (df['Total Length of Bwd Packets'] + 1)  # Adding 1 to avoid division by zero\n",
    "\n",
    "    # 14. PSH and URG Flag Ratios\n",
    "    total_packets = df['Total Fwd Packets'] + df['Total Backward Packets']\n",
    "    df['PSH_Flag_Ratio'] = (df['Fwd PSH Flags'] + df['Bwd PSH Flags']) / total_packets\n",
    "    df['URG_Flag_Ratio'] = (df['Fwd URG Flags'] + df['Bwd URG Flags']) / total_packets\n",
    "\n",
    "    # 15. Average Packet Size\n",
    "    df['Avg_Packet_Size'] = (df['Total Length of Fwd Packets'] + df['Total Length of Bwd Packets']) / total_packets\n",
    "\n",
    "    # 16. Calculate Entropy for Packet Lengths\n",
    "    df['Packet_Length_Entropy'] = df['Total Length of Fwd Packets'].apply(calculate_entropy)\n",
    "\n",
    "    # 17. Group By Source IP and Aggregate Metrics\n",
    "    df_grouped = df.groupby('Source IP').agg({\n",
    "        'Total Length of Fwd Packets': 'sum',\n",
    "        'Total Length of Bwd Packets': 'sum',\n",
    "        'SYN Flag Count': 'sum',\n",
    "        'Protocol': 'max',\n",
    "        'Flow Duration': 'mean',\n",
    "        'Flow IAT Mean': 'mean',\n",
    "        'Packet_Length_Entropy': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Multithreading for geolocation using local database\n",
    "    reader = geoip2.database.Reader(db_path)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_ip = {executor.submit(get_country, ip, reader): ip for ip in df['Source IP']}\n",
    "        for future in as_completed(future_to_ip):\n",
    "            ip = future_to_ip[future]\n",
    "            try:\n",
    "                country = future.result()\n",
    "                df.loc[df['Source IP'] == ip, 'Country'] = country\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving country for IP {ip}: {e}\")\n",
    "\n",
    "    # Save the enhanced DataFrame to a CSV file if an output path is provided\n",
    "    if output_path:\n",
    "        df_grouped.to_csv(output_path, index=False, encoding='latin1')\n",
    "    \n",
    "    return df_grouped\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = r'E:\\LogicInsights\\works\\network_data.csv'\n",
    "    db_path = r'path\\to\\GeoLite2-City.mmdb'\n",
    "    output_path = r'E:\\LogicInsights\\works\\output.csv'\n",
    "    \n",
    "    # Process the CSV and get the enhanced DataFrame\n",
    "    df_enhanced = process_network_csv(csv_path, db_path, output_path)\n",
    "    \n",
    "    # Print a summary of the processed data\n",
    "    print(df_enhanced.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch --pre -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kani2\\AppData\\Local\\Temp\\ipykernel_19928\\2934347932.py:18: DtypeWarning: Columns (0,1,3,6,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path, encoding='latin1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the input CSV file after stripping spaces: Index(['Flow ID', 'Source IP', 'Source Port', 'Destination IP',\n",
      "       'Destination Port', 'Protocol', 'Timestamp', 'Flow Duration',\n",
      "       'Total Fwd Packets', 'Total Backward Packets',\n",
      "       'Total Length of Fwd Packets', 'Total Length of Bwd Packets',\n",
      "       'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
      "       'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
      "       'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
      "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s',\n",
      "       'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
      "       'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
      "       'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
      "       'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags',\n",
      "       'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
      "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
      "       'Min Packet Length', 'Max Packet Length', 'Packet Length Mean',\n",
      "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
      "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
      "       'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
      "       'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size',\n",
      "       'Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk',\n",
      "       'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk',\n",
      "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
      "       'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
      "       'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward',\n",
      "       'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
      "       'Idle Std', 'Idle Max', 'Idle Min', 'Label'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLogicInsights\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mworks\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Process the CSV and get the enhanced DataFrame\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df_enhanced \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_network_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Print a summary of the processed data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_enhanced\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[23], line 115\u001b[0m, in \u001b[0;36mprocess_network_csv\u001b[1;34m(csv_path, db_path, output_path)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     country \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m--> 115\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSource IP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCountry\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m country\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError retrieving country for IP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mip\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kani2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kani2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1942\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1941\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\kani2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1970\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1967\u001b[0m ilocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_iterable_column_indexer(info_axis)\n\u001b[0;32m   1969\u001b[0m pi \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1970\u001b[0m lplane_indexer \u001b[38;5;241m=\u001b[39m \u001b[43mlength_of_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;66;03m# lplane_indexer gives the expected length of obj[indexer[0]]\u001b[39;00m\n\u001b[0;32m   1972\u001b[0m \n\u001b[0;32m   1973\u001b[0m \u001b[38;5;66;03m# we need an iterable, with a ndim of at least 1\u001b[39;00m\n\u001b[0;32m   1974\u001b[0m \u001b[38;5;66;03m# eg. don't pass through np.array(0)\u001b[39;00m\n\u001b[0;32m   1975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like_indexer(value) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\kani2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexers\\utils.py:323\u001b[0m, in \u001b[0;36mlength_of_indexer\u001b[1;34m(indexer, target)\u001b[0m\n\u001b[0;32m    319\u001b[0m         indexer \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(indexer)\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m indexer\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;66;03m# GH#25774\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexer)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mrange\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\kani2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_path = r\"C:\\Users\\kani2\\Downloads\\Datasets\\GeneratedLabelledFlows (1)\\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\"\n",
    "db_path = r'GeoLite2-City_20240723\\GeoLite2-City_20240723\\GeoLite2-City.mmdb'\n",
    "output_path = r'E:\\LogicInsights\\works\\output.csv'\n",
    "\n",
    "# Process the CSV and get the enhanced DataFrame\n",
    "df_enhanced = process_network_csv(csv_path, db_path, output_path)\n",
    "\n",
    "# Print a summary of the processed data\n",
    "print(df_enhanced.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted C:\\Users\\kani2\\Downloads\\GeoLite2-City_20240723.tar.gz to C:\\Users\\kani2\\Downloads\\GeoLite2-City_20240723\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "def extract_tar_gz(file_path, output_dir):\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        tar.extractall(output_dir)\n",
    "    print(f\"Successfully extracted {file_path} to {output_dir}\")\n",
    "\n",
    "extract_tar_gz(r\"C:\\Users\\kani2\\Downloads\\GeoLite2-City_20240723.tar.gz\", r\"C:\\Users\\kani2\\Downloads\\GeoLite2-City_20240723\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
